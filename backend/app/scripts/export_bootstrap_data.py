"""
export_bootstrap_data.py

Export current database state, FAISS index, and Elasticsearch index for bootstrap.
Creates a compressed bundle that can be used for fast first-time setup.

Output structure:
/app/data/bootstrap/
  - persistent_candidates.pgdump  (PostgreSQL custom format dump)
  - faiss_index.bin               (FAISS HNSW index)
  - faiss_map.json                (ID mapping)
  - elasticsearch_mapping.json    (ES index mapping)
  - metadata.json                 (export metadata: counts, timestamp, version)

Usage:
    docker exec -i watchbuddy-backend-1 sh -c "cd /app && PYTHONPATH=/app python app/scripts/export_bootstrap_data.py"
"""

import logging
import json
import subprocess
import shutil
from pathlib import Path
from datetime import datetime
from sqlalchemy import text

from app.core.database import SessionLocal
from app.services.elasticsearch_client import get_elasticsearch_client

logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")
logger = logging.getLogger(__name__)

# Paths
BOOTSTRAP_DIR = Path("/app/data/bootstrap")
FAISS_DIR = Path("/data/ai")
OUTPUT_BUNDLE = Path("/app/data/watchbuddy_bootstrap.tar.gz")


def export_database():
    """Export persistent_candidates table using COPY to SQL file."""
    logger.info("Exporting persistent_candidates table...")
    
    sql_file = BOOTSTRAP_DIR / "persistent_candidates.sql"
    
    db = SessionLocal()
    try:
        # Get table structure
        logger.info("Exporting table structure...")
        with open(sql_file, 'w', encoding='utf-8') as f:
            # Write header
            f.write("-- WatchBuddy persistent_candidates export\n")
            f.write("-- Generated by export_bootstrap_data.py\n\n")
            
            # Truncate table first (for clean import)
            f.write("TRUNCATE TABLE persistent_candidates CASCADE;\n\n")
        
        # Use COPY TO to export data (binary format, compressed)
        copy_file = BOOTSTRAP_DIR / "persistent_candidates.copy"
        
        # Export using COPY in binary format
        copy_query = """
            COPY persistent_candidates TO STDOUT WITH (FORMAT binary)
        """
        
        logger.info("Exporting table data (binary format)...")
        raw_conn = db.connection().connection  # Get raw psycopg2 connection
        cursor = raw_conn.cursor()
        
        with open(copy_file, 'wb') as f:
            cursor.copy_expert(copy_query, f)
        
        # Get count for verification
        count = db.execute(text("SELECT COUNT(*) FROM persistent_candidates")).scalar()
        
        # Compress the copy file
        import gzip
        compressed_file = BOOTSTRAP_DIR / "persistent_candidates.copy.gz"
        with open(copy_file, 'rb') as f_in:
            with gzip.open(compressed_file, 'wb', compresslevel=6) as f_out:
                f_out.writelines(f_in)
        
        # Remove uncompressed file
        copy_file.unlink()
        
        # Write import SQL
        with open(sql_file, 'a', encoding='utf-8') as f:
            f.write(f"-- Table contains {count} rows\n")
            f.write("-- Import with: gunzip -c persistent_candidates.copy.gz | psql -U watchbuddy -d watchbuddy -c \"COPY persistent_candidates FROM STDIN WITH (FORMAT binary)\"\n")
        
        size_mb = compressed_file.stat().st_size / (1024 * 1024)
        logger.info(f"✅ Database exported: {count} rows, {size_mb:.2f} MB (compressed)")
        return compressed_file
        
    finally:
        db.close()


def export_faiss_index():
    """Copy FAISS index files to bootstrap directory."""
    logger.info("Copying FAISS index files...")
    
    index_file = FAISS_DIR / "faiss_index.bin"
    map_file = FAISS_DIR / "faiss_map.json"
    
    if not index_file.exists() or not map_file.exists():
        logger.warning("FAISS index not found, skipping")
        return None, None
    
    dest_index = BOOTSTRAP_DIR / "faiss_index.bin"
    dest_map = BOOTSTRAP_DIR / "faiss_map.json"
    
    shutil.copy2(index_file, dest_index)
    shutil.copy2(map_file, dest_map)
    
    index_size_mb = dest_index.stat().st_size / (1024 * 1024)
    map_size_mb = dest_map.stat().st_size / (1024 * 1024)
    
    logger.info(f"✅ FAISS index: {index_size_mb:.2f} MB, mapping: {map_size_mb:.2f} MB")
    return dest_index, dest_map


def export_elasticsearch_mapping():
    """Export Elasticsearch index mapping (not the data, just structure)."""
    logger.info("Exporting Elasticsearch mapping...")
    
    es_client = get_elasticsearch_client()
    
    if not es_client.is_connected():
        logger.warning("Elasticsearch not connected, skipping mapping export")
        return None
    
    try:
        # Import INDEX_NAME constant
        from app.services.elasticsearch_client import INDEX_NAME
        
        # Get index mapping
        mapping = es_client.es.indices.get_mapping(index=INDEX_NAME)
        
        mapping_file = BOOTSTRAP_DIR / "elasticsearch_mapping.json"
        with open(mapping_file, 'w') as f:
            json.dump(mapping, f, indent=2)
        
        logger.info(f"✅ Elasticsearch mapping exported: {mapping_file}")
        return mapping_file
        
    except Exception as e:
        logger.warning(f"Failed to export ES mapping: {e}")
        return None


def collect_metadata():
    """Collect metadata about the export."""
    logger.info("Collecting export metadata...")
    
    db = SessionLocal()
    try:
        # Count candidates
        total_candidates = db.execute(
            text("SELECT COUNT(*) FROM persistent_candidates WHERE active=true")
        ).scalar()
        
        candidates_with_embeddings = db.execute(
            text("SELECT COUNT(*) FROM persistent_candidates WHERE active=true AND embedding IS NOT NULL")
        ).scalar()
        
        candidates_with_trakt = db.execute(
            text("SELECT COUNT(*) FROM persistent_candidates WHERE active=true AND trakt_id IS NOT NULL")
        ).scalar()
        
        # Media type breakdown
        movie_count = db.execute(
            text("SELECT COUNT(*) FROM persistent_candidates WHERE active=true AND media_type='movie'")
        ).scalar()
        
        tv_count = db.execute(
            text("SELECT COUNT(*) FROM persistent_candidates WHERE active=true AND media_type='tv'")
        ).scalar()
        
        metadata = {
            "export_timestamp": datetime.utcnow().isoformat(),
            "version": "1.0",
            "counts": {
                "total_candidates": total_candidates,
                "with_embeddings": candidates_with_embeddings,
                "with_trakt_id": candidates_with_trakt,
                "movies": movie_count,
                "tv_shows": tv_count
            },
            "components": {
                "database": True,
                "faiss_index": (BOOTSTRAP_DIR / "faiss_index.bin").exists(),
                "faiss_mapping": (BOOTSTRAP_DIR / "faiss_map.json").exists(),
                "elasticsearch_mapping": (BOOTSTRAP_DIR / "elasticsearch_mapping.json").exists()
            }
        }
        
        metadata_file = BOOTSTRAP_DIR / "metadata.json"
        with open(metadata_file, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        logger.info(f"✅ Metadata: {total_candidates} candidates, {candidates_with_embeddings} with embeddings")
        return metadata_file
        
    finally:
        db.close()


def create_bundle():
    """Create compressed tar.gz bundle of all bootstrap files."""
    logger.info("Creating bootstrap bundle...")
    
    # Create tar.gz with all files in bootstrap directory
    cmd = [
        "tar",
        "-czf", str(OUTPUT_BUNDLE),
        "-C", str(BOOTSTRAP_DIR.parent),
        BOOTSTRAP_DIR.name
    ]
    
    try:
        subprocess.run(cmd, check=True)
        bundle_size_mb = OUTPUT_BUNDLE.stat().st_size / (1024 * 1024)
        logger.info(f"✅ Bundle created: {OUTPUT_BUNDLE} ({bundle_size_mb:.2f} MB)")
        return OUTPUT_BUNDLE
    except subprocess.CalledProcessError as e:
        logger.error(f"tar failed: {e}")
        raise


def main():
    """Main export process."""
    logger.info("=" * 60)
    logger.info("WatchBuddy Bootstrap Data Export")
    logger.info("=" * 60)
    
    # Create bootstrap directory
    BOOTSTRAP_DIR.mkdir(parents=True, exist_ok=True)
    
    # Export components
    export_database()
    export_faiss_index()
    export_elasticsearch_mapping()
    collect_metadata()
    
    # Create bundle
    bundle = create_bundle()
    
    # Clean up temporary directory
    shutil.rmtree(BOOTSTRAP_DIR)
    
    logger.info("=" * 60)
    logger.info(f"✅ Export complete: {bundle}")
    logger.info("To use this bundle, place it in /app/data/ on new installations")
    logger.info("=" * 60)


if __name__ == "__main__":
    main()
